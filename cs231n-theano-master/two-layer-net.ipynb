{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (3073, 49000)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (3073, 1000)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (3073, 1000)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))]).T\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))]).T\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))]).T\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot(x,n):\n",
    "    if type(x) == list: x = np.array(x)\n",
    "    x = x.flatten()\n",
    "    o_h = np.zeros((len(x),n))\n",
    "    o_h[np.arange(len(x)),x] = 1\n",
    "    return o_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels shape:  (49000, 10)\n",
      "Train labels shape:  (1000, 10)\n",
      "Train labels shape:  (1000, 10)\n",
      "Train data shape:  (49000, 3073)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Test data shape:  (1000, 3073)\n"
     ]
    }
   ],
   "source": [
    "y_train_o_h = one_hot(y_train,10)\n",
    "y_val_o_h = one_hot(y_val,10)\n",
    "y_test_o_h = one_hot(y_test,10)\n",
    "print 'Train labels shape: ', y_train_o_h.shape\n",
    "print 'Train labels shape: ', y_val_o_h.shape\n",
    "print 'Train labels shape: ', y_test_o_h.shape\n",
    "\n",
    "X_train_t = np.transpose(X_train)\n",
    "X_val_t = np.transpose(X_val)\n",
    "X_test_t = np.transpose(X_test)\n",
    "print 'Train data shape: ', X_train_t.shape\n",
    "print 'Validation data shape: ', X_val_t.shape\n",
    "print 'Test data shape: ', X_test_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '29464' (I am process '31414')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/naoki/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '29464' (I am process '31414')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/naoki/.theano/compiledir_Linux-4.4--generic-x86_64-with-Ubuntu-16.04-xenial-x86_64-2.7.12-64/lock_dir\n",
      "Using gpu device 2: GeForce GTX 1080 (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape, factor=0.00001):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * factor))\n",
    "\n",
    "def sgd(cost, params, lr=8.910000e-07):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        updates.append([p, p - g * lr])\n",
    "    return updates\n",
    "\n",
    "def model(X, w_h, w_o):\n",
    "    h = T.nnet.hard_sigmoid(T.dot(X, w_h))\n",
    "    return T.nnet.softmax(T.dot(h, w_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((3073, 70))\n",
    "w_o = init_weights((70, 10))\n",
    "\n",
    "py_x = model(X, w_h, w_o)\n",
    "y_pred = T.argmax(py_x, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y)) + 0.5 * 2.610000e+01 * ((w_h * w_h).sum() + (w_o * w_o).sum())\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "params = [w_h, w_o]\n",
    "updates = sgd(cost, params, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Error: 0.414\n",
      "0 0.398\n",
      "1 0.39\n",
      "2 0.403\n",
      "3 0.41\n",
      "4 0.391\n",
      "5 0.397\n",
      "6 0.4\n",
      "7 0.389\n",
      "8 0.397\n",
      "9 0.397\n",
      "10 0.397\n",
      "11 0.398\n",
      "12 0.395\n",
      "13 0.4\n",
      "14 0.4\n",
      "15 0.405\n",
      "16 0.404\n",
      "17 0.412\n",
      "18 0.391\n",
      "19 0.396\n"
     ]
    }
   ],
   "source": [
    "print \"Initial Error:\", np.mean(np.argmax(y_test_o_h, axis=1) == predict(X_test_t))\n",
    "for i in range(20):\n",
    "    for start, end in zip(range(0, len(X_train_t), 128), range(128, len(X_train_t), 128)):\n",
    "        cost = train(X_train_t[start:end], y_train_o_h[start:end])\n",
    "    print i, np.mean(np.argmax(y_test_o_h, axis=1) == predict(X_test_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(n_hidden_units, learning_rate, num_epochs, regularization_strength):\n",
    "    X = T.fmatrix()\n",
    "    Y = T.fmatrix()\n",
    "    w_h = init_weights((3073, n_hidden_units))\n",
    "    w_o = init_weights((n_hidden_units, 10))\n",
    "    py_x = model(X, w_h, w_o)\n",
    "    y_pred = T.argmax(py_x, axis=1)\n",
    "    cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y)) + 0.5 * regularization_strength * ((w_h * w_h).sum() + (w_o * w_o).sum())\n",
    "    params = [w_h, w_o]\n",
    "    updates = sgd(cost, params, lr=learning_rate)\n",
    "    train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "    predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "    for i in range(num_epochs):\n",
    "        for start, end in zip(range(0, len(X_train_t), 128), range(128, len(X_train_t), 128)):\n",
    "            cost = train(X_train_t[start:end], y_train_o_h[start:end])\n",
    "    return np.mean(np.argmax(y_val_o_h, axis=1) == predict(X_val_t))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "settings = {}\n",
    "best_val = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. evaluating: # hidden units = 500, lr = 0.002589, # epochs = 6, reg = 0.1392 --->>> 0.429\n",
      "2. evaluating: # hidden units = 450, lr = 0.005034, # epochs = 6, reg = 0.116 --->>> 0.427\n",
      "3. evaluating: # hidden units = 500, lr = 0.004449, # epochs = 6, reg = 0.07981 --->>> 0.443\n",
      "4. evaluating: # hidden units = 500, lr = 0.007192, # epochs = 6, reg = 0.1459 --->>> 0.431\n",
      "5. evaluating: # hidden units = 450, lr = 0.004753, # epochs = 6, reg = 0.1209 --->>> 0.434\n",
      "6. evaluating: # hidden units = 500, lr = 0.007277, # epochs = 6, reg = 0.1327 --->>> 0.421\n",
      "7. evaluating: # hidden units = 550, lr = 0.004006, # epochs = 6, reg = 0.1478 --->>> 0.441\n",
      "8. evaluating: # hidden units = 350, lr = 0.006646, # epochs = 6, reg = 0.08071 --->>> 0.419\n",
      "9. evaluating: # hidden units = 550, lr = 0.004749, # epochs = 6, reg = 0.1207 --->>> 0.442\n",
      "10. evaluating: # hidden units = 500, lr = 0.005215, # epochs = 6, reg = 0.1484 --->>> 0.437\n",
      "\n",
      "-----\n",
      "Best Settings: # hidden units = 500, lr = 0.004449, # epochs = 6, reg = 0.07981 -->> 0.443\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "while cnt < 10:\n",
    "    # Best Settings: # hidden units = 300, lr = 0.004318, # epochs = 6, reg = 0.1245 -->> 0.428\n",
    "    # Best Settings: # hidden units = 500, lr = 0.00517, # epochs = 6, reg = 0.0835 -->> 0.434\n",
    "    cnt += 1\n",
    "    n_hidden_units = np.random.choice(range(300,650,50))\n",
    "    #n_hidden_units = 250\n",
    "    learning_rate = np.random.uniform(0.002, 0.0075)\n",
    "    #learning_rate = 0.008\n",
    "    #num_epochs = np.random.choice(range(10,60,10))\n",
    "    num_epochs = 6\n",
    "    regularization_strength = np.random.uniform(0.075, 0.15)\n",
    "    #regularization_strength = 0.04\n",
    "\n",
    "    print \"%d. evaluating: # hidden units = %d, lr = %0.4g, # epochs = %d, reg = %0.4g\" % (cnt, n_hidden_units, learning_rate, num_epochs, regularization_strength),\n",
    "    val = evaluate(n_hidden_units, learning_rate, num_epochs, regularization_strength)\n",
    "    if val > best_val:\n",
    "        best_val = val\n",
    "        settings[best_val] = [n_hidden_units, learning_rate, num_epochs, regularization_strength]\n",
    "    print \"--->>>\", val\n",
    "best = settings[best_val]\n",
    "print \"\\n-----\\nBest Settings: # hidden units = %d, lr = %0.4g, # epochs = %d, reg = %0.4g -->> %0.3f\" % (\n",
    "    best[0], best[1], best[2], best[3], best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.447\n"
     ]
    }
   ],
   "source": [
    "val = evaluate(500, 0.006, 100, 0.1)\n",
    "print val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
